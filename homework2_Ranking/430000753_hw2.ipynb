{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 2:  PageRank + Learning to Rank\n",
    "\n",
    "### 100 points [10% of your final grade]\n",
    "\n",
    "### Due: March 5, 2020 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will explore real-world challenges of building a graph (in this case, from tweets), implement and test the classic PageRank algortihm over this graph. In addition, you will apply learning to rank to a real-world dataset and report the performance in terms of NDCG.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PageRank (60 points)\n",
    "In this assignment, we're going to adapt the classic PageRank approach to allow us to find not the most authoritative web pages, but rather to find significant Twitter users. \n",
    "\n",
    "\n",
    "## Part 1.1: A re-Tweet Graph (20 points)\n",
    "\n",
    "So, instead of viewing the world as web pages with hyperlinks (where pages = nodes, hyperlinks = edges), we're going to construct a graph of Twitter users and their retweets of other Twitter users (so user = node, retweet of another user = edge). Over this Twitter-user graph, we can apply the PageRank approach to order the users. The main idea is that a user who is retweeted by other users is more \"impactful\". \n",
    "\n",
    "Here is a toy example. Suppose you are given the following four retweets:\n",
    "\n",
    "* **userID**: diane, **text**: \"RT \", **sourceID**: bob\n",
    "* **userID**: charlie, **text**: \"RT Welcome\", **sourceID**: alice\n",
    "* **userID**: bob, **text**: \"RT Hi \", **sourceID**: diane\n",
    "* **userID**: alice, **text**: \"RT Howdy!\", **sourceID**: parisa\n",
    "\n",
    "There are four short tweets retweeted by four users. The retweet between users form a directed graph with five nodes and four edges. E.g., the \"diane\" node has a directed edge to the \"bob\" node.\n",
    "\n",
    "You should build a graph by parsing the tweets in the file we provide called *PageRank.json*.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* You may see some weird characters in the content of tweets, just ignore them. \n",
    "* The edges are binary and directed. If Bob retweets Alice once, in 10 tweets, or 10 times in one tweet, there is an edge from Bob to Alice, but there is not an edge from Alice to Bob.\n",
    "* If a user retweets herself, ignore it.\n",
    "* Correctly parsing screen_name in a tweet is error-prone. Use the id of the user (this is the user who is re-tweeting) and the id of the user in the retweeted_status field (this is the user who is being re-tweeted; that is, this user created the original tweet).\n",
    "* Later you will need to implement the PageRank algorithm on the graph you build here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import copy\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pagerank = {}\n",
    "\n",
    "num_nodes = 0\n",
    "num_edges = 0\n",
    "d = 0.9\n",
    "\n",
    "edge_in = defaultdict(set)\n",
    "edge_out = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here define your function for building the graph by parsing \n",
    "# the input file of tweets\n",
    "# Insert as many cells as you want\n",
    "\n",
    "def graph_generate():\n",
    "    global num_edges\n",
    "    with open ('pagerank.json/hits.json',encoding=\"utf8\") as d_file:\n",
    "        for line in d_file:\n",
    "            tweet = json.loads(line)\n",
    "            user_id = tweet[\"user\"][\"id\"]\n",
    "            op_user_id = tweet[\"retweeted_status\"][\"user\"][\"id\"]\n",
    "            if user_id == op_user_id:\n",
    "                continue\n",
    "            edge_out[user_id].add(op_user_id)\n",
    "            if op_user_id not in edge_out:\n",
    "                edge_out[op_user_id] = set()\n",
    "            edge_in[op_user_id].add(user_id)\n",
    "            if user_id not in edge_in:\n",
    "                edge_in[user_id] = set()\n",
    "\n",
    "def print_graph_meta():\n",
    "    global num_nodes\n",
    "    global num_edges\n",
    "    num_nodes = len(edge_out)\n",
    "    print( \"Total nodes: \" + str(num_nodes))\n",
    "    num_edges = 0\n",
    "    for node in edge_out.keys():\n",
    "        num_edges += len(edge_out[node])\n",
    "    print (\"Total Edges : \" + str(num_edges))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 1003\n",
      "Total Edges : 6177\n"
     ]
    }
   ],
   "source": [
    "# Call your function to print out the size of the graph, \n",
    "# i.e., the number of nodes and edges\n",
    "# How you maintain the graph is totaly up to you\n",
    "# However, if you encounter any memory issues, we recommend you \n",
    "#write the graph into a file, and load it later.\n",
    "graph_generate()\n",
    "print_graph_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not check the correctness of your graph. However, this will affect the PageRank results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: PageRank Implementation (30 points)\n",
    "\n",
    "Your program will return the top 10 users with highest PageRank scores. The **output** should be like:\n",
    "\n",
    "* user1 - score1\n",
    "* user2 - score2\n",
    "* ...\n",
    "* user10 - score10\n",
    "\n",
    "You should follow these **rules**:\n",
    "\n",
    "* Assume all nodes start out with equal probability.\n",
    "* The probability of the random surfer teleporting is 0.1 (that is, the damping factor is 0.9).\n",
    "* If a user is never retweeted and does not retweet anyone, their PageRank scores should be zero. Do not include the user in the calculation.\n",
    "* It is up to you to decide when to terminate the PageRank calculation.\n",
    "* There are PageRank implementations out there on the web. Remember, your code should be **your own**.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "* If you're using the matrix style approach, you should use [numpy.matrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html).\n",
    "* Scipy is built on top of Numpy and has support for sparse matrices. You most likely will not need to use Scipy unless you'd like to try out their sparse matrices.\n",
    "* If you choose to use Numpy (and Scipy), please make sure your Anaconda environment include their latest versions.\n",
    "* Test your parsing and PageRank calculations using a handful of tweets, before moving on to the entire file we provide.\n",
    "* We will evaluate the user ranks you provide as well as the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the termination condition in your PageRank implementation? Describe it below:\n",
    "\n",
    "I check the page rank score of 50 nodes. If the page rank score dosent change in an iteration, we conclude that convergence has been achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk(high):\n",
    "    temp = 0\n",
    "    for w in sorted(dict_pagerank, key=dict_pagerank.get, reverse=True):\n",
    "        if temp == high:\n",
    "            break\n",
    "        print( w, dict_pagerank[w])\n",
    "        temp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(present, previous):\n",
    "    count = 50\n",
    "    if previous == None:\n",
    "        return False\n",
    "    for i in range(count):\n",
    "        if present[i][0] != previous[i][0]:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here add your code to implement a function called PageRanker\n",
    "# Insert as many cells as you want\n",
    "\n",
    "def PageRanker():\n",
    "    present = []\n",
    "    previous = None\n",
    "    for node in edge_out.keys():\n",
    "        dict_pagerank[node] = 1.0 / num_nodes\n",
    "    iter = 0\n",
    "    while True:\n",
    "        iter += 1\n",
    "        temp_copy = copy.deepcopy(dict_pagerank)\n",
    "        for node in edge_out.keys():\n",
    "            dict_pagerank[node] = 0.0\n",
    "            for incoming_node in edge_in[node]:\n",
    "                total_len = len(edge_out[incoming_node])\n",
    "                preempt_len = temp_copy[incoming_node]\n",
    "                dict_pagerank[node] += float(preempt_len) / total_len\n",
    "            dict_pagerank[node] *= d\n",
    "            dict_pagerank[node] += (1 - d) / num_nodes\n",
    "        present = sorted(dict_pagerank.items(), key = operator.itemgetter(1), reverse=True)\n",
    "        flag = check_convergence(present, previous)\n",
    "        if flag == True:\n",
    "            break\n",
    "        else:\n",
    "            previous = present\n",
    "    count_pr = sum(dict_pagerank.values())\n",
    "    for key, value in dict_pagerank.items():\n",
    "        dict_pagerank[key] = float(value)/count_pr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183906148 0.028884376712513107\n",
      "3019659587 0.021570090805795614\n",
      "3077695572 0.020951644836023964\n",
      "3068694151 0.01850905491727246\n",
      "2598548166 0.017946993839813852\n",
      "3154266823 0.0174823129374862\n",
      "3042570996 0.01737600593411384\n",
      "571198546 0.017205680082834674\n",
      "3039321886 0.015524345039655479\n",
      "3082766914 0.01462300657234436\n"
     ]
    }
   ],
   "source": [
    "# Now let's call your function on the graph you've built. Output the results.\n",
    "PageRanker()\n",
    "get_topk(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Improving PageRank (10 points)\n",
    "In the many years since PageRank was introduced, there have been many improvements and extensions. For this part, you should experiment with one such improvement and then compare the results you get with the original results in Part 1.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 1003\n",
      "Total Edges : 6177\n",
      "3077695572 0.025246428474644228\n",
      "3019659587 0.024718522012607137\n",
      "571198546 0.024206629243985787\n",
      "3068694151 0.021847448360243868\n",
      "1183906148 0.021826427189460373\n",
      "3039321886 0.01831122397752903\n",
      "3082766914 0.01765680886983887\n",
      "258262535 0.015297650674578228\n",
      "1638625987 0.015112680338183256\n",
      "3082049188 0.014436365553555925\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Mar  7 16:38:08 2020\n",
    "\n",
    "@author: rizuj\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import copy\n",
    "import operator\n",
    "import json\n",
    "\n",
    "dict_pagerank_plus = {}\n",
    "\n",
    "num_nodes_plus = 0\n",
    "num_edges_plus = 0\n",
    "d = 0.9\n",
    "\n",
    "edge_in_plus = defaultdict(set)\n",
    "edge_out_plus = defaultdict(set)\n",
    "\n",
    "status_count = defaultdict(int)\n",
    "\n",
    "# Here define your function for building the graph by parsing \n",
    "# the input file of tweets\n",
    "# Insert as many cells as you want\n",
    "\n",
    "def graph_generate_plus():\n",
    "    global num_edges_plus\n",
    "    with open ('pagerank.json/hits.json',encoding=\"utf8\") as d_file:\n",
    "        for line in d_file:\n",
    "            tweet = json.loads(line)\n",
    "            user_id = tweet[\"user\"][\"id\"]\n",
    "            op_user_id = tweet[\"retweeted_status\"][\"user\"][\"id\"]\n",
    "            if user_id == op_user_id:\n",
    "                continue\n",
    "            edge_out_plus[user_id].add(op_user_id)\n",
    "            status_count[user_id] = tweet[\"user\"][\"statuses_count\"]\n",
    "            user_id = tweet[\"user\"][\"id\"]\n",
    "\n",
    "            if op_user_id not in edge_out_plus:\n",
    "                edge_out_plus[op_user_id] = set()\n",
    "            edge_in_plus[op_user_id].add(user_id)\n",
    "            if user_id not in edge_in_plus:\n",
    "                edge_in_plus[user_id] = set()\n",
    "\n",
    "def print_graph_meta_plus():\n",
    "    global num_nodes_plus\n",
    "    global num_edges_plus\n",
    "    num_nodes_plus = len(edge_out_plus)\n",
    "    print( \"Total nodes: \" + str(num_nodes_plus))\n",
    "    num_edges_plus = 0\n",
    "    for node in edge_out_plus.keys():\n",
    "        num_edges_plus += len(edge_out_plus[node])\n",
    "    print (\"Total Edges : \" + str(num_edges_plus))             \n",
    "\n",
    "def get_topk_plus(high):\n",
    "    temp = 0\n",
    "    for w in sorted(dict_pagerank_plus, key=dict_pagerank_plus.get, reverse=True):\n",
    "        if temp == high:\n",
    "            break\n",
    "        print( w, dict_pagerank_plus[w])\n",
    "        temp += 1\n",
    "\n",
    "\n",
    "def check_convergence_plus(present, previous):\n",
    "    count = 10\n",
    "    if previous == None:\n",
    "        return False\n",
    "    for i in range(count):\n",
    "        if present[i][0] != previous[i][0]:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def PageRanker_plus():\n",
    "    present = []\n",
    "    previous = None\n",
    "    for node in edge_out_plus.keys():\n",
    "        dict_pagerank_plus[node] = 1.0 / num_nodes_plus\n",
    "    iter = 0\n",
    "    max_status_count = max(status_count.values())\n",
    "    while True:\n",
    "        iter += 1\n",
    "        temp_copy = copy.deepcopy(dict_pagerank_plus)\n",
    "        for node in edge_out_plus.keys():\n",
    "            dict_pagerank_plus[node] = 0.0\n",
    "            for incoming_node in edge_in_plus[node]:\n",
    "                total_len = len(edge_out_plus[incoming_node])\n",
    "                preempt_len = temp_copy[incoming_node]\n",
    "                dict_pagerank_plus[node] += float(preempt_len) / total_len\n",
    "            dict_pagerank_plus[node] *= d\n",
    "            dict_pagerank_plus[node] += (1 - d) / num_nodes_plus\n",
    "            dict_pagerank_plus[node] += (0.5 + (status_count[node]/2*max_status_count))\n",
    "        present = sorted(dict_pagerank_plus.items(), key = operator.itemgetter(1), reverse=True)\n",
    "        flag = check_convergence_plus(present, previous)\n",
    "        if flag == True:\n",
    "            break\n",
    "        else:\n",
    "            previous = present\n",
    "    count_pr = sum(dict_pagerank_plus.values())\n",
    "    for key, value in dict_pagerank_plus.items():\n",
    "        dict_pagerank_plus[key] = float(value)/count_pr\n",
    "                \n",
    "\n",
    "graph_generate_plus()\n",
    "print_graph_meta_plus()\n",
    "PageRanker_plus()\n",
    "get_topk_plus(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Plus be sure to describe your extension (what is it? why did you choose it?) and your comparison to Part 1.2\n",
    "\n",
    "The idea is that the user who gives a lot of statuses has potentially a lot of noise. So, in order to reduce noise and provide users with more quality feed, our pagerank algorithm should take into consideration the no. of tweets a person makes. To model this, we remember the status count of each node and after each iteration we factor the pageranks down by the status count. We get a new set of top users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning to Rank (40 points)\n",
    "\n",
    "For this part, we're going to play with some Microsoft LETOR data that has query-document relevance judgments. Let's see how learning to rank works in practice. \n",
    "\n",
    "First, you will need to download the MQ2008.zip file from the Resources tab on Piazza. This is data from the [Microsoft Research IR Group](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/).\n",
    "\n",
    "The data includes 15,211 rows. Each row is a query-document pair. The first column is a relevance label of this pair (0,1 or 2--> the higher value the more related), the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. A query-document pair is represented by a 46-dimensional feature vector. Features are a numeric value describing a document and query such as TFIDF, BM25, Page Rank, .... You can find compelete description of features from [here](https://arxiv.org/ftp/arxiv/papers/1306/1306.2597.pdf).\n",
    "\n",
    "The good news for you is the dataset is ready for analysis: It has already been split into 5 folds (see the five folders called Fold1, ..., Fold5).\n",
    "\n",
    "\n",
    "## Part 2.1: Build Point-wise Learning to Rank  (20 points)\n",
    "First, you should build a point-wise Learning to Rank framework. \n",
    "1. You could train a binary classification model like SVM or logistic regression on the train file. In this case, 0 is treated as negative (irrelevant) sample and 1, 2 are treated as positive (relevant) sample.\n",
    "2. You apply the already trained model to predict the scores for documents on test file.\n",
    "3. Order the documents based on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the data\n",
    "# 0 -> Query id\n",
    "# 1 -> Document id\n",
    "# 2-47 -> Features\n",
    "# 48 -> ranking\n",
    "# 49 -> label\n",
    "def transformLETOR(input_df):\n",
    "    input_df[96] = input_df[0]\n",
    "    input_df[0] = input_df[2]\n",
    "    input_df[2] = input_df[97]\n",
    "    drop_cols = list(range(1,96,2))\n",
    "    drop_cols.extend(range(97,104))\n",
    "    input_df_trans = input_df.drop(drop_cols,1)\n",
    "    input_df_trans.columns = list(range(0,49))\n",
    "    input_df_trans[49] = input_df_trans[48] > 0\n",
    "    input_df_trans.infer_objects()\n",
    "    input_df_trans[49] = input_df_trans[49].apply(int)\n",
    "    \n",
    "    return input_df_trans\n",
    "\n",
    "    \n",
    "\n",
    "# Create a SVC classifier using an RBF kernel\n",
    "def learn_to_rank(Folder):\n",
    "    \n",
    "    train_df_raw = pd.read_csv(Folder + \"train.txt\",\" |:\",header=None,engine='python')\n",
    "    train_df = transformLETOR(train_df_raw)\n",
    "    \n",
    "    val_df_raw = pd.read_csv(Folder + \"vali.txt\",\" |:\",header=None, engine='python')\n",
    "    val_df = transformLETOR(val_df_raw)\n",
    "    \n",
    "    test_df_raw = pd.read_csv(Folder + \"test.txt\",\" |:\",header=None, engine='python')\n",
    "    test_df = transformLETOR(test_df_raw)\n",
    "    \n",
    "    X_train = train_df.iloc[:,2:48]\n",
    "    Y_train = train_df.iloc[:,49]\n",
    "    \n",
    "    X_val = val_df.iloc[:,2:48]\n",
    "    \n",
    "    X_test = test_df.iloc[:,2:48]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    C = [1e-5,1e-4,1e-3,1e-2,1e-1,1e0,1e1]\n",
    "    \n",
    "    for hyp in C:\n",
    "        logreg_class = LogisticRegression(C=hyp,max_iter = 500, class_weight = 'balanced',solver='lbfgs')\n",
    "        \n",
    "        # Train the classifier\n",
    "        logreg_class.fit(X_train, Y_train)\n",
    "        predictions = logreg_class.predict_proba(X_val)\n",
    "        val_df[51] = predictions[:,1]\n",
    "        \n",
    "        val_ndgc = NDCG(val_df)\n",
    "        results.append(val_ndgc)\n",
    "    \n",
    "    best_hyp = results.index(max(results))\n",
    "    logreg_class = LogisticRegression(C=C[best_hyp],max_iter = 500, class_weight = 'balanced',solver='lbfgs')\n",
    "    logreg_class.fit(X_train, Y_train)\n",
    "    predictions = logreg_class.predict_proba(X_test)\n",
    "    test_df[51] = predictions[:,1]\n",
    "    test_ndgc = NDCG(test_df)\n",
    "    \n",
    "    return test_ndgc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using Logistic Regression for learning a classifier. The approach used to tune the hyperparameter is to fit on training data and predict validation data. The best NDGC score for validation data is the winner and we compute the test scores with that hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: NDCG (20 points)\n",
    "\n",
    "Based on your prediction file (results could be ranked by scores in the prediction file) and ground-truth (i.e., 0,1,2) in the test file, calculate NDCG for each query. Report average NDCG for all queries in the five-fold cross validation.\n",
    "\n",
    "For NDCG, please bulid your own function rather then using any package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# pred_df needs to have query_id,document_id,score\n",
    "def NDCG(pred_df):\n",
    "    final = pred_df.sort_values([0,51],ascending=[False, False])    \n",
    "\n",
    "    query_id = final[0][0]\n",
    "    user_rel = []\n",
    "    \n",
    "    ndgc = 0\n",
    "    query_count = 0\n",
    "    for j in range(final.shape[0]):\n",
    "        if query_id != final[0][j]:\n",
    "            # get ndcg\n",
    "            dgc = 0\n",
    "            for k in range(min(10,len(user_rel))):\n",
    "                dgc += user_rel[k]/(math.log(k+2))\n",
    "                \n",
    "            ideal_rel = sorted(user_rel,reverse=True)\n",
    "            idgc = 0\n",
    "            \n",
    "            for k in range(min(10,len(user_rel))):\n",
    "                idgc += (ideal_rel[k])/(math.log(k+2))                \n",
    "            \n",
    "            if idgc:\n",
    "                ndgc += (dgc/idgc)\n",
    "            \n",
    "            query_id = final[0][j]\n",
    "            user_rel = [final[48][j]]\n",
    "            query_count += 1\n",
    "        else:\n",
    "            if len(user_rel)<10:\n",
    "                user_rel.append(final[48][j])\n",
    "    \n",
    "    return ndgc/query_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDGC for fold1:  0.36612722571674816\n",
      "NDGC for fold2:  0.3939720988916711\n",
      "NDGC for fold3:  0.36553542096652986\n",
      "NDGC for fold4:  0.4383451079899545\n",
      "NDGC for fold5:  0.421025156153727\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1,6):\n",
    "    directory = \"MQ2008/MQ2008/Fold\"+str(idx)+\"/\"\n",
    "    dir_ndgc = learn_to_rank(directory)\n",
    "    print(\"NDGC for fold\"+str(idx)+\": \",dir_ndgc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Pairwise Learning to Rank (5 points)\n",
    "\n",
    "Rather than use the point-wise approach as in Part 2.1, instead try to implement a paiwise approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
