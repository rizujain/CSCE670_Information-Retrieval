{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 4:  Word Embeddings for Information Retrieval and Query Expansion\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: April 28, 2020 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will improve your information retrieval engine in homework 1 by word embeddings to: (i) directly match the query and the document in the latent semantic space of word embeddings; (ii) expand the original query via word embeddings.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw4.ipynb`. For example, my homework submission would be something like `555001234_hw4.ipynb`. Submit this notebook via eCampus (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Dataset and Parsing (The same as Homework 1)\n",
    "\n",
    "The dataset is collected from Quizlet (https://quizlet.com), a website where users can generated their own flashcards. Each flashcard generated by a user is made up of an entity on the front and a definition describing or explaining the entity correspondingly on the back. We treat entities on each flashcard's front as the queries and the definitions on the back of flashcards as the documents. Definitions (documents) are relevant to an entity (query) if the definitions are from the back of the entity's flashcard; otherwise definitions are not relevant. **In this homework, queries and entities are interchangeable as well as documents and definitions.**\n",
    "\n",
    "The format of the dataset is like this:\n",
    "\n",
    "**query \\t document id \\t document**\n",
    "\n",
    "Examples:\n",
    "\n",
    "decision tree\t\\t 27946 \\t\tshow complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\n",
    "\n",
    "where \"decision tree\" is the entity in the front of a flashcard and \"show complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\" is the definition on the flashcard's back and \"27946\" is the id of the definition. Naturally, this document is relevant to the query.\n",
    "\n",
    "false positive rate\t\\t 686\t\\t fall-out; probability of a false alarm\n",
    "\n",
    "where document 686 is not relevant to query \"decision tree\" because the entity of \"fall-out; probability of a false alarm\" is \"false positive rate\".\n",
    "\n",
    "For parsing this dataset, you could also just copy your code from homework 1 to complete the following tasks:\n",
    "* Tokenize documents (definitions) using **whitespaces and punctuations as delimiters**.\n",
    "* Remove stop words: use nltk stop words list (from nltk.corpus import stopwords)\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "* Remove any other strings that you think are less informative or nosiy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration options\n",
    "remove_stopwords = True\n",
    "use_stemming = True\n",
    "remove_otherNoise = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your parser function here. It will take the three option variables above as the parameters\n",
    "# It does the inital pre processing except for removal of stopwords. \n",
    "# Stopwords are removed after the inverted index is built.\n",
    "\n",
    "def myParser(word):\n",
    "    \n",
    "    # handling leading and trailing hyphens\n",
    "    word = word.strip(\"-\")\n",
    "    \n",
    "    # Case Folding\n",
    "    word = word.lower()            \n",
    "    \n",
    "    if use_stemming == True:\n",
    "        # Stem the words user Porter's Algorithm\n",
    "        myStemmer = PorterStemmer()\n",
    "        word = myStemmer.stem(word)\n",
    "                \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dictionary:  9122\n"
     ]
    }
   ],
   "source": [
    "# Build Dataset\n",
    "# contains the documents indexed by their  ids\n",
    "# list with 2 fields, [0] -> definition [1] -> Entitiy\n",
    "documents = {}\n",
    "\n",
    "# document tokens indexed by their ids to avoid redundant processing\n",
    "doc_tokens = {}\n",
    "\n",
    "Total_document_length = 0\n",
    "\n",
    "# inverted index for our corpus\n",
    "inverted_index = {}\n",
    "\n",
    "# word frequency for ranking the doucments\n",
    "# 0 -> term frequency in the corpus\n",
    "word_frequency = {}\n",
    "\n",
    "# prepare a vocabulary index for the column id of individual words\n",
    "vocab_id_table = {}\n",
    "vocab_id = 0\n",
    "\n",
    "# get a document id list for our queries\n",
    "query_ground_truth = {\"relational database\":[], \"garbage collection\":[],\"retrieval model\":[]}\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stopword_count  = 0\n",
    "with open('homework_1_data.txt', 'r', encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc = line.split('\\t')\n",
    "        curr_doc_id = int(doc[1])\n",
    "\n",
    "        if doc[0] in query_ground_truth:\n",
    "            query_ground_truth[doc[0]].append(curr_doc_id)\n",
    "\n",
    "            # ids converted to integer for faster matching\n",
    "        documents[curr_doc_id] = [doc[2],doc[0]]\n",
    "\n",
    "        doc[2] = ''.join(c if c.isalpha() else ' 'for c in doc[2] )\n",
    "\n",
    "        if remove_otherNoise == True:        \n",
    "            # Remove irrelevant punctuations\n",
    "            remove_punct = '!\"#$&\\'()*+,./;:<=>?@[\\\\]^`{|}~%_-'\n",
    "            table = str.maketrans(remove_punct, ' '*len(remove_punct))\n",
    "            doc[2] = doc[2].translate(table)\n",
    "\n",
    "        words_stop = doc[2].split()\n",
    "        words = []\n",
    "        \n",
    "        for word in words_stop:\n",
    "            word = word.lower()\n",
    "            \n",
    "            if word in stopwords_set:\n",
    "                continue\n",
    "            \n",
    "            if len(word) < 3:\n",
    "                continue\n",
    "            \n",
    "            words.append(word)\n",
    "                \n",
    "        Total_document_length += len(words)\n",
    "\n",
    "        # the set of seen words for each document\n",
    "        for i in range(len(words)):\n",
    "\n",
    "            words[i] = myParser(words[i])\n",
    "\n",
    "            # is this word in our dictionary already?\n",
    "            if words[i] in inverted_index:\n",
    "\n",
    "                # add the current document id to that word's document list\n",
    "                inverted_index[words[i]].add(curr_doc_id)\n",
    "\n",
    "            # seeing the word for the first time\n",
    "            else:\n",
    "                # add the current document id to that word's document list\n",
    "                inverted_index[words[i]] = set({curr_doc_id})\n",
    "\n",
    "                # returns the id for individual words\n",
    "                vocab_id_table[words[i]] = vocab_id\n",
    "                vocab_id += 1\n",
    "\n",
    "        # store the tokens for this document\n",
    "        doc_tokens[curr_doc_id] = words\n",
    "\n",
    "# total number of documents in the corpus\n",
    "N = len(doc_tokens)\n",
    "\n",
    "print(\"Size of the dictionary: \",len(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## prepare vector space for documents\n",
    "#\n",
    "\n",
    "doc_vectors = np.zeros((N,vocab_id))\n",
    "IDF = {}\n",
    "IDF_BM25 = {}\n",
    "\n",
    "for word,word_id in vocab_id_table.items():\n",
    "    nq = len(inverted_index[word])\n",
    "    IDF[word] = math.log10(N/nq)\n",
    "    IDF_BM25[word] = math.log10((N - nq + 0.5)/(nq + 0.5))\n",
    "\n",
    "for this_doc_id,this_doc_tokens in doc_tokens.items():\n",
    "\n",
    "    # count the number of occurences for ech word        \n",
    "    term_freq = Counter(this_doc_tokens)\n",
    "    # for each word and its count\n",
    "    for term,count in term_freq.items():\n",
    "        tf = 1 + math.log10(count)\n",
    "        # documents are rows and words are columns\n",
    "        doc_vectors[this_doc_id][vocab_id_table[term]] += (tf * IDF[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word2Vec (30 points)\n",
    "\n",
    "In this part you will use the Word2Vec algorithm to generate word embeddings for tokens in the dataset. You can just use a package like https://radimrehurek.com/gensim/models/word2vec.html. Let's set the size of word embeddings to be 20. Please print the word embeddings for the tokens: \n",
    "* relational\n",
    "* database\n",
    "* garbage\n",
    "* collection\n",
    "* retrieval \n",
    "* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here.\n",
    "import gensim.models\n",
    "embedding_size = 20\n",
    "model = gensim.models.Word2Vec(sentences =list(doc_tokens.values()),size=embedding_size,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Relational\n",
      " [ 2.2680657   0.06140756 -0.44610825 -1.1656089  -0.84581745 -0.88587624\n",
      "  1.3794376  -2.1949944   0.44469178  0.4344101   2.9320416  -0.91699606\n",
      "  2.0353422   0.4220861   0.9019404  -0.2977019   2.06147     2.64691\n",
      " -1.083384    0.5937177 ]\n",
      "-------------------------------------------------------------------\n",
      "Database\n",
      " [ 1.5953257  -0.5497807   0.39062363 -2.8846865  -1.8896031   1.4535891\n",
      "  0.47864187 -2.3056474   3.1766396  -0.26811945  2.465781   -1.5395129\n",
      "  1.417544    0.3847839  -0.8906662  -0.20475644  2.1688192   2.7075443\n",
      " -1.8384706   0.0804768 ]\n",
      "-------------------------------------------------------------------\n",
      "Garbage\n",
      " [ 0.23651579 -0.27403742 -0.15699883  0.16288637  0.01649721 -0.00773037\n",
      " -0.00263683 -0.17949635  0.36617285 -0.10970375 -0.01249154 -0.08075885\n",
      "  0.19968687  0.00279451  0.12102219  0.04184372  0.4077469   0.04915323\n",
      " -0.36516207 -0.17871958]\n",
      "-------------------------------------------------------------------\n",
      "Collection\n",
      " [ 0.7429559  -0.34870204 -1.0657978  -2.1991992  -1.0579696  -0.09273181\n",
      " -1.5828596  -1.572786    1.2649071  -1.3006077   2.0794199  -0.03571221\n",
      "  0.6558112  -0.24062824  0.9766012  -0.15286778  1.1490645   2.0928714\n",
      " -2.6997259  -0.24106972]\n",
      "-------------------------------------------------------------------\n",
      "Retrieval\n",
      " [ 0.34777346 -0.7882323  -0.10265368 -2.046001   -0.26889288  0.9043233\n",
      " -1.7015676  -2.1893554   3.329686   -0.03650339  1.5298799  -0.24522188\n",
      "  0.46609914  1.4942864   0.62411726 -1.2464126   1.064864    0.93484217\n",
      " -2.3524818  -0.21178572]\n",
      "-------------------------------------------------------------------\n",
      "Model\n",
      " [-0.08790917 -2.0415576   0.5750801  -0.6887523   0.69083405 -0.16175976\n",
      "  1.8659618  -0.90499926  0.23963933 -0.8638836   2.1082444  -0.7010803\n",
      "  0.6779425  -1.4952298   1.1242594   2.4183424   2.5038161   0.16378658\n",
      "  0.51235014  2.6507726 ]\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the word embeddings of the six tokens\n",
    "relational = model.wv[myParser(\"relational\")]\n",
    "database = model.wv[myParser(\"database\")]\n",
    "garbage = model.wv[myParser(\"garbage\")]\n",
    "collection = model.wv[myParser(\"collection\")]\n",
    "retrieval = model.wv[myParser(\"retrieval\")]\n",
    "model_vec = model.wv[myParser(\"model\")]\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Relational\\n\" , relational)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Database\\n\", database)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Garbage\\n\",garbage)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Collection\\n\", collection)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Retrieval\\n\", retrieval)\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Model\\n\",model_vec)\n",
    "print(\"-------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Vector Space Model via Word Embeddings (40 points) \n",
    "\n",
    "In this part, your job is to match the query and the document via the cosine similarity between the embeddings of them.\n",
    "\n",
    "Since there are not just one token in a query or a document, the first challenge is how to aggregate many word embeddings into one embedding of a query or a document. There are many ways to do so: \n",
    "* Max pooling: return the maximum value along each dimension of a bunch of word embeddings. For example, [1, 3, 4], [2, 1, 5] -> [2, 3, 5].\n",
    "* Min pooling: return the minimum value along each dimension of a bunch of word embeddings\n",
    "* Mean pooling: return the mean value along each dimension of a bunch of word embeddings\n",
    "* Sum: element-wise add a bunch of word embeddings together\n",
    "* Weighted sum: assign weights to word embeddings and then add them together. Weights could be TF, IDF or TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "max_pool_docvec = np.zeros((len(doc_tokens),embedding_size))\n",
    "min_pool_docvec = np.zeros((len(doc_tokens),embedding_size))\n",
    "mean_pool_docvec = np.zeros((len(doc_tokens),embedding_size))\n",
    "sum_docvec = np.zeros((len(doc_tokens),embedding_size))\n",
    "weighted_pool_docvec = np.zeros((len(doc_tokens),embedding_size))\n",
    "\n",
    "for doc_id,tokens in doc_tokens.items():\n",
    "    if len(tokens)<1:\n",
    "        continue\n",
    "    doc_embeddings = np.zeros((len(tokens),embedding_size))\n",
    "\n",
    "    for pos,token in enumerate(tokens):\n",
    "        # the tokens are already parsed\n",
    "        if token in model.wv:\n",
    "            doc_embeddings[pos,:] = model.wv[token]\n",
    "        \n",
    "        weighted_pool_docvec[doc_id,:] += doc_embeddings[pos,:] * doc_vectors[doc_id,vocab_id_table[token]]\n",
    "\n",
    "    max_pool_docvec[doc_id,:] = np.max(doc_embeddings,axis=0)\n",
    "    min_pool_docvec[doc_id,:] = np.min(doc_embeddings,axis=0)\n",
    "    mean_pool_docvec[doc_id,:] = np.mean(doc_embeddings,axis=0)\n",
    "    sum_docvec[doc_id,:] = np.sum(doc_embeddings,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different aggregation methods and report the precision@10 for these queries:\n",
    "* query: relational database\n",
    "* query: garbage collection\n",
    "* query: retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['relational database','garbage collection','retrieval model']\n",
    "\n",
    "max_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "min_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "mean_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "sum_qvec = np.zeros((len(queries),embedding_size))\n",
    "weighted_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "\n",
    "for q_id,query in enumerate(queries):\n",
    "    qtokens = query.split(' ')\n",
    "    q_embeddings = np.zeros((len(qtokens),embedding_size))\n",
    "\n",
    "    for qpos,qtoken in enumerate(qtokens):\n",
    "        # the tokens are already parsed\n",
    "        q_embeddings[qpos,:] = model.wv[myParser(qtoken)]\n",
    "        weighted_pool_qvec[q_id,:] += model.wv[myParser(qtoken)] * IDF[myParser(qtoken)]\n",
    "    \n",
    "    max_pool_qvec[q_id,:] = np.max(q_embeddings,axis=0)\n",
    "    min_pool_qvec[q_id,:] = np.min(q_embeddings,axis=0)\n",
    "    mean_pool_qvec[q_id,:] = np.mean(q_embeddings,axis=0)\n",
    "    sum_qvec[q_id,:] = np.sum(q_embeddings,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10\n",
      "\n",
      "relational database  Max pooling:  0.3\n",
      "relational database  Min pooling:  0.5\n",
      "relational database  Mean pool:  0.5\n",
      "relational database  sum:  0.0\n",
      "relational database  weighted sum:  0.0\n",
      "\n",
      "garbage collection  Max pooling:  0.0\n",
      "garbage collection  Min pooling:  0.0\n",
      "garbage collection  Mean pool:  0.0\n",
      "garbage collection  sum:  0.0\n",
      "garbage collection  weighted sum:  0.0\n",
      "\n",
      "retrieval model  Max pooling:  0.0\n",
      "retrieval model  Min pooling:  0.0\n",
      "retrieval model  Mean pool:  0.0\n",
      "retrieval model  sum:  0.0\n",
      "retrieval model  weighted sum:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_sim_maxpool = dot(max_pool_docvec,max_pool_qvec.T)\n",
    "cos_sim_minpool = dot(min_pool_docvec,min_pool_qvec.T)\n",
    "cos_sim_meanpool = dot(mean_pool_docvec,mean_pool_qvec.T)\n",
    "cos_sim_sum = dot(sum_docvec,sum_qvec.T)\n",
    "cos_sim_tfidf = dot(weighted_pool_docvec,weighted_pool_qvec.T)\n",
    "\n",
    "print(\"Precision@10\")\n",
    "print()\n",
    "for q_id,query in enumerate(queries):\n",
    "    # gets the top 10 in linear time\n",
    "    top10_maxpool = set(np.argpartition(cos_sim_maxpool[:,q_id].T, -10)[-10:])\n",
    "    top10_minpool = set(np.argpartition(cos_sim_minpool[:,q_id].T, -10)[-10:])\n",
    "    top10_meanpool = set(np.argpartition(cos_sim_meanpool[:,q_id].T, -10)[-10:])\n",
    "    top10_sum = set(np.argpartition(cos_sim_sum[:,q_id].T, -10)[-10:])\n",
    "    top10_tfidf = set(np.argpartition(cos_sim_tfidf[:,q_id].T, -10)[-10:])\n",
    "        \n",
    "    gt = set(query_ground_truth[queries[q_id]])\n",
    "    \n",
    "    z = top10_maxpool.intersection(gt)\n",
    "    print(query, \" Max pooling: \",len(z)/10)\n",
    "\n",
    "    z = top10_minpool.intersection(gt)\n",
    "    print(query, \" Min pooling: \",len(z)/10)\n",
    "\n",
    "    z = top10_meanpool.intersection(gt)\n",
    "    print(query, \" Mean pool: \",len(z)/10)\n",
    "    \n",
    "    z = top10_sum.intersection(gt)\n",
    "    print(query, \" sum: \",len(z)/10)\n",
    "\n",
    "    z = top10_tfidf.intersection(gt)\n",
    "    print(query, \" weighted sum: \",len(z)/10)\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Among these aggregation methods, which one is the best and which one is the worst?\n",
    "\n",
    "Given the dataset isnt comprehensive or large enough to effectively learn word vectors, the realiablity and observations of the results is questionable.\n",
    "\n",
    "Nevertheless, Amongst these, the maximum and minimum pooling methods seems to be performing the best.\n",
    "\n",
    "Summing methods doesnt give the best results in our vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Query Expansion via Word Embeddings (30 points) \n",
    "Remember the hardest query \"retrieval model\" in homework 1? Because there is no document containing \"retrieval model\" in the dataset, you cannot retrieve any documents by Boolean matching. Now, it is the time of your \"revenge\" via query expansion.\n",
    "\n",
    "In this part, your job is to expand the original query like \"retrieval model\" by adding semantically similar words (e.g., \"search\"), which are selected from all tokens in the dataset.\n",
    "\n",
    "There are many ways to do so. For this part, we want you to calculate the cosine similarity between each of the original query tokens and the other tokens based on their word embeddings.\n",
    "\n",
    "First, please find the top 3 similar tokens for:\n",
    "* relational\n",
    "* database\n",
    "* garbage\n",
    "* collection\n",
    "* retrieval \n",
    "* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relational ->  ['entiti', 'tabl', 'rdb']\n",
      "database ->  ['dbm', 'rdbm', 'entir']\n",
      "garbage ->  ['later', 'yet', 'egg']\n",
      "collection ->  ['repositori', 'gather', 'warehous']\n",
      "retrieval ->  ['store', 'updat', 'warehous']\n",
      "model ->  ['mathemat', 'formal', 'diagram']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "expanded_queries = [query.split() for query in queries]\n",
    "\n",
    "for q_id, query in enumerate(queries):\n",
    "    for token in query.split(' '):\n",
    "        expand = model.wv.most_similar(positive=[myParser(token)], topn=3)\n",
    "        expand = [j[0] for j in expand]\n",
    "        print(token,\"-> \",expand)\n",
    "        expanded_queries[q_id].extend(expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report recall@10 before the query expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 (Before expanding queries)\n",
      "\n",
      "relational database  Max pooling:  0.01056338028169014\n",
      "relational database  Min pooling:  0.017605633802816902\n",
      "relational database  Mean pool:  0.017605633802816902\n",
      "relational database  sum:  0.0\n",
      "relational database  weighted sum:  0.0\n",
      "\n",
      "garbage collection  Max pooling:  0.0\n",
      "garbage collection  Min pooling:  0.0\n",
      "garbage collection  Mean pool:  0.0\n",
      "garbage collection  sum:  0.0\n",
      "garbage collection  weighted sum:  0.0\n",
      "\n",
      "retrieval model  Max pooling:  0.0\n",
      "retrieval model  Min pooling:  0.0\n",
      "retrieval model  Mean pool:  0.0\n",
      "retrieval model  sum:  0.0\n",
      "retrieval model  weighted sum:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "max_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "min_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "mean_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "sum_qvec = np.zeros((len(queries),embedding_size))\n",
    "weighted_pool_qvec = np.zeros((len(queries),embedding_size))\n",
    "\n",
    "for q_id,query in enumerate(queries):\n",
    "    qtokens = query.split(' ')\n",
    "    q_embeddings = np.zeros((len(qtokens),embedding_size))\n",
    "\n",
    "    for qpos,qtoken in enumerate(qtokens):\n",
    "        # the tokens are already parsed\n",
    "        q_embeddings[qpos,:] = model.wv[myParser(qtoken)]\n",
    "        weighted_pool_qvec[q_id,:] += model.wv[myParser(qtoken)] * IDF[myParser(qtoken)]\n",
    "    \n",
    "    max_pool_qvec[q_id,:] = np.max(q_embeddings,axis=0)\n",
    "    min_pool_qvec[q_id,:] = np.min(q_embeddings,axis=0)\n",
    "    mean_pool_qvec[q_id,:] = np.mean(q_embeddings,axis=0)\n",
    "    sum_qvec[q_id,:] = np.sum(q_embeddings,axis=0)\n",
    "\n",
    "cos_sim_maxpool = dot(max_pool_docvec,max_pool_qvec.T)\n",
    "cos_sim_minpool = dot(min_pool_docvec,min_pool_qvec.T)\n",
    "cos_sim_meanpool = dot(mean_pool_docvec,mean_pool_qvec.T)\n",
    "cos_sim_sum = dot(sum_docvec,sum_qvec.T)\n",
    "cos_sim_tfidf = dot(weighted_pool_docvec,weighted_pool_qvec.T)\n",
    "\n",
    "print(\"Recall@10 (Before expanding queries)\")\n",
    "print()\n",
    "for q_id,query in enumerate(queries):\n",
    "    # gets the top 10 in linear time\n",
    "    top10_maxpool = set(np.argpartition(cos_sim_maxpool[:,q_id].T, -10)[-10:])\n",
    "    top10_minpool = set(np.argpartition(cos_sim_minpool[:,q_id].T, -10)[-10:])\n",
    "    top10_meanpool = set(np.argpartition(cos_sim_meanpool[:,q_id].T, -10)[-10:])\n",
    "    top10_sum = set(np.argpartition(cos_sim_sum[:,q_id].T, -10)[-10:])\n",
    "    top10_tfidf = set(np.argpartition(cos_sim_tfidf[:,q_id].T, -10)[-10:])\n",
    "        \n",
    "    gt = set(query_ground_truth[queries[q_id]])\n",
    "    \n",
    "    z = top10_maxpool.intersection(gt)\n",
    "    print(query, \" Max pooling: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_minpool.intersection(gt)\n",
    "    print(query, \" Min pooling: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_meanpool.intersection(gt)\n",
    "    print(query, \" Mean pool: \",len(z)/len(gt))\n",
    "    \n",
    "    z = top10_sum.intersection(gt)\n",
    "    print(query, \" sum: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_tfidf.intersection(gt)\n",
    "    print(query, \" weighted sum: \",len(z)/len(gt))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report recall@10 after the query expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 (After expanding queries)\n",
      "\n",
      "Original:  ['relational', 'database']\n",
      "Expanded:  ['relational', 'database', 'entiti', 'tabl', 'rdb', 'dbm', 'rdbm', 'entir']\n",
      "\n",
      "relational database  Max pooling:  0.0\n",
      "relational database  Min pooling:  0.0\n",
      "relational database  Mean pool:  0.017605633802816902\n",
      "relational database  sum:  0.0\n",
      "relational database  weighted sum:  0.0\n",
      "\n",
      "Original:  ['garbage', 'collection']\n",
      "Expanded:  ['garbage', 'collection', 'later', 'yet', 'egg', 'repositori', 'gather', 'warehous']\n",
      "\n",
      "garbage collection  Max pooling:  0.0\n",
      "garbage collection  Min pooling:  0.0\n",
      "garbage collection  Mean pool:  0.0\n",
      "garbage collection  sum:  0.0\n",
      "garbage collection  weighted sum:  0.0\n",
      "\n",
      "Original:  ['retrieval', 'model']\n",
      "Expanded:  ['retrieval', 'model', 'store', 'updat', 'warehous', 'mathemat', 'formal', 'diagram']\n",
      "\n",
      "retrieval model  Max pooling:  0.0\n",
      "retrieval model  Min pooling:  0.0\n",
      "retrieval model  Mean pool:  0.0\n",
      "retrieval model  sum:  0.0\n",
      "retrieval model  weighted sum:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "max_pool_qvec = np.zeros((len(expanded_queries),embedding_size))\n",
    "min_pool_qvec = np.zeros((len(expanded_queries),embedding_size))\n",
    "mean_pool_qvec = np.zeros((len(expanded_queries),embedding_size))\n",
    "sum_qvec = np.zeros((len(expanded_queries),embedding_size))\n",
    "weighted_pool_qvec = np.zeros((len(expanded_queries),embedding_size))\n",
    "\n",
    "for q_id,qtokens in enumerate(expanded_queries):\n",
    "    q_embeddings = np.zeros((len(qtokens),embedding_size))\n",
    "\n",
    "    for qpos,qtoken in enumerate(qtokens):\n",
    "        # the tokens are already parsed\n",
    "        q_embeddings[qpos,:] = model.wv[myParser(qtoken)]\n",
    "        weighted_pool_qvec[q_id,:] += model.wv[myParser(qtoken)] * IDF[myParser(qtoken)]\n",
    "    \n",
    "    max_pool_qvec[q_id,:] = np.max(q_embeddings,axis=0)\n",
    "    min_pool_qvec[q_id,:] = np.min(q_embeddings,axis=0)\n",
    "    mean_pool_qvec[q_id,:] = np.mean(q_embeddings,axis=0)\n",
    "    sum_qvec[q_id,:] = np.sum(q_embeddings,axis=0)\n",
    "\n",
    "cos_sim_maxpool = dot(max_pool_docvec,max_pool_qvec.T)\n",
    "cos_sim_minpool = dot(min_pool_docvec,min_pool_qvec.T)\n",
    "cos_sim_meanpool = dot(mean_pool_docvec,mean_pool_qvec.T)\n",
    "cos_sim_sum = dot(sum_docvec,sum_qvec.T)\n",
    "cos_sim_tfidf = dot(weighted_pool_docvec,weighted_pool_qvec.T)\n",
    "\n",
    "print(\"Recall@10 (After expanding queries)\")\n",
    "print()\n",
    "for q_id,query in enumerate(queries):\n",
    "    print(\"Original: \",query.split())\n",
    "    print(\"Expanded: \",expanded_queries[q_id])\n",
    "    print()\n",
    "\n",
    "    # gets the top 10 in linear time\n",
    "    top10_maxpool = set(np.argpartition(cos_sim_maxpool[:,q_id].T, -10)[-10:])\n",
    "    top10_minpool = set(np.argpartition(cos_sim_minpool[:,q_id].T, -10)[-10:])\n",
    "    top10_meanpool = set(np.argpartition(cos_sim_meanpool[:,q_id].T, -10)[-10:])\n",
    "    top10_sum = set(np.argpartition(cos_sim_sum[:,q_id].T, -10)[-10:])\n",
    "    top10_tfidf = set(np.argpartition(cos_sim_tfidf[:,q_id].T, -10)[-10:])\n",
    "        \n",
    "    gt = set(query_ground_truth[queries[q_id]])\n",
    "    \n",
    "    z = top10_maxpool.intersection(gt)\n",
    "    print(query, \" Max pooling: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_minpool.intersection(gt)\n",
    "    print(query, \" Min pooling: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_meanpool.intersection(gt)\n",
    "    print(query, \" Mean pool: \",len(z)/len(gt))\n",
    "    \n",
    "    z = top10_sum.intersection(gt)\n",
    "    print(query, \" sum: \",len(z)/len(gt))\n",
    "\n",
    "    z = top10_tfidf.intersection(gt)\n",
    "    print(query, \" weighted sum: \",len(z)/len(gt))\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Why we measure recall here instead of precision or NDCG?\n",
    "Getting ground truth ranking scores is tough in the given dataset. And without that, we are not effectively implementing NDCG\n",
    "acccurately. The intutions from this metric could be misleading and hence we opt for recall on this dataset.\n",
    "\n",
    "Should the tokens added for expansion have the same importance as the original query tokens? If not, how to improve the query expansion in this part?\n",
    "No, the added tokens should not have the same importance as the orignal query tokens as they arent what the user explicitly asked for.\n",
    "To improve, we can do a weighted sum where each of the newly added vectors are multiplied by its cosine similarity with the query words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
